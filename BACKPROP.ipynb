{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "88nGcEFqef0Q"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24_l7Oyoef0V"
   },
   "source": [
    "\n",
    "`Learn the Basics <intro.html>`_ ||\n",
    "`Quickstart <quickstart_tutorial.html>`_ ||\n",
    "`Tensors <tensorqs_tutorial.html>`_ ||\n",
    "`Datasets & DataLoaders <data_tutorial.html>`_ ||\n",
    "`Transforms <transforms_tutorial.html>`_ ||\n",
    "`Build Model <buildmodel_tutorial.html>`_ ||\n",
    "`Autograd <autogradqs_tutorial.html>`_ ||\n",
    "**Optimization** ||\n",
    "`Save & Load Model <saveloadrun_tutorial.html>`_\n",
    "\n",
    "Optimizing Model Parameters\n",
    "===========================\n",
    "\n",
    "Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on\n",
    "our data. Training a model is an iterative process; in each iteration (called an *epoch*) the model makes a guess about the output, calculates\n",
    "the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in\n",
    "the `previous section  <autograd_tutorial.html>`_), and **optimizes** these parameters using gradient descent. For a more\n",
    "detailed walkthrough of this process, check out this video on `backpropagation from 3Blue1Brown <https://www.youtube.com/watch?v=tIeHLnjs5U8>`__.\n",
    "\n",
    "Prerequisite Code\n",
    "-----------------\n",
    "We load the code from the previous sections on `Datasets & DataLoaders <data_tutorial.html>`_\n",
    "and `Build Model  <buildmodel_tutorial.html>`_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "q2T-db9Fef0b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "# model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9aiB-a7ljZSQ"
   },
   "outputs": [],
   "source": [
    "class FirstModel(nn.Module): #For images\n",
    "    def __init__(self):\n",
    "        super(FirstModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8RgFeaHwgPo_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SecondModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.Softmax(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Softmax(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Softmax(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DaY0GAfvggol"
   },
   "outputs": [],
   "source": [
    "# class Combined_model(nn.Module):\n",
    "#     def __init__(self, modelA, modelB):\n",
    "#         super(Combined_model, self).__init__()\n",
    "#         self.modelA = modelA\n",
    "#         self.modelB = modelB\n",
    "#         self.classifier = nn.Linear(20, 10)\n",
    "        \n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.modelA(x1)\n",
    "#         x2 = self.modelB(x2)\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uCh3TbUtJGo1"
   },
   "outputs": [],
   "source": [
    "class ThirdModel(nn.Module): #For images\n",
    "    def __init__(self):\n",
    "        super(ThirdModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MnMaaTL4QOFP"
   },
   "outputs": [],
   "source": [
    "class Combined_modelInt(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Combined_modelInt, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.classifier = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.modelA(x1)\n",
    "        x2 = self.modelB(x2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ed7oLXMNQS9H"
   },
   "outputs": [],
   "source": [
    "class Combined_modelBruh(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Combined_modelBruh, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.classifier = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.modelA(x1)\n",
    "        x2 = self.modelB(x2, x2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OKn_p4JzhwYn"
   },
   "outputs": [],
   "source": [
    "M1=FirstModel()\n",
    "M3=ThirdModel()\n",
    "M2=SecondModel()\n",
    "M4 = Combined_modelInt(M1, M3)\n",
    "# input_image = torch.rand(3,28,28)\n",
    "# print(input_image.shape)\n",
    "# model1 = Combined_modelInt(M1, M2)\n",
    "# # x1, x2 = torch.randn(1, 10), torch.randn(1, 20)\n",
    "# output1 = model1(input_image, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4KwgcTmyhwkm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3uvU-frhwuT",
    "outputId": "ea4cd5ce-5e30-4b26-9c8e-9f28a966ae13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "# print(output1.shape)\n",
    "model = Combined_modelBruh(M2, M4)\n",
    "# x1, x2 = torch.randn(1, 10), torch.randn(1, 20)\n",
    "output = model(input_image, input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "frMENd5lhw6D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PffTEMSef0e"
   },
   "source": [
    "Hyperparameters\n",
    "-----------------\n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process.\n",
    "Different hyperparameter values can impact model training and convergence rates\n",
    "(`read more <https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>`__ about hyperparameter tuning)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    " - **Number of Epochs** - the number times to iterate over the dataset\n",
    " - **Batch Size** - the number of data samples propagated through the network before the parameters are updated\n",
    " - **Learning Rate** - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OZh1y1SIhvR_"
   },
   "outputs": [],
   "source": [
    "M1=FirstModel()\n",
    "M3=ThirdModel()\n",
    "M2=SecondModel()\n",
    "M4 = Combined_modelInt(M2, M3)\n",
    "model = Combined_modelBruh(M1, M4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Xi2JLQ-7ef0g"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TZUU5Aabef0k"
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "c-moZ0qDef0r"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IsN9sc9gef0y"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X,X)\n",
    "        # pred_label = torch.max(pred, dim = 1)\n",
    "        # correct = (pred_label == y).float()\n",
    "        \n",
    "        # print(pred_label.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X,X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-T1hFV3ef0z"
   },
   "source": [
    "We initialize the loss function and optimizer, and pass it to ``train_loop`` and ``test_loop``.\n",
    "Feel free to increase the number of epochs to track the model's improving performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU-MZeTSef00",
    "outputId": "d9e28933-dc85-4915-be9e-cb50f4ec15b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.313334  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.308922  [ 6400/60000]\n",
      "loss: 2.319726  [12800/60000]\n",
      "loss: 2.321332  [19200/60000]\n",
      "loss: 2.294420  [25600/60000]\n",
      "loss: 2.293205  [32000/60000]\n",
      "loss: 2.306473  [38400/60000]\n",
      "loss: 2.295124  [44800/60000]\n",
      "loss: 2.313856  [51200/60000]\n",
      "loss: 2.309300  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.306882 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.310316  [    0/60000]\n",
      "loss: 2.306760  [ 6400/60000]\n",
      "loss: 2.315878  [12800/60000]\n",
      "loss: 2.317416  [19200/60000]\n",
      "loss: 2.294489  [25600/60000]\n",
      "loss: 2.293016  [32000/60000]\n",
      "loss: 2.304900  [38400/60000]\n",
      "loss: 2.294872  [44800/60000]\n",
      "loss: 2.310818  [51200/60000]\n",
      "loss: 2.307293  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.305010 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.307800  [    0/60000]\n",
      "loss: 2.305031  [ 6400/60000]\n",
      "loss: 2.312657  [12800/60000]\n",
      "loss: 2.314136  [19200/60000]\n",
      "loss: 2.294666  [25600/60000]\n",
      "loss: 2.292946  [32000/60000]\n",
      "loss: 2.303612  [38400/60000]\n",
      "loss: 2.294726  [44800/60000]\n",
      "loss: 2.308284  [51200/60000]\n",
      "loss: 2.305582  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 13.5%, Avg loss: 2.303460 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.305721  [    0/60000]\n",
      "loss: 2.303613  [ 6400/60000]\n",
      "loss: 2.309910  [12800/60000]\n",
      "loss: 2.311387  [19200/60000]\n",
      "loss: 2.294871  [25600/60000]\n",
      "loss: 2.292901  [32000/60000]\n",
      "loss: 2.302514  [38400/60000]\n",
      "loss: 2.294627  [44800/60000]\n",
      "loss: 2.306078  [51200/60000]\n",
      "loss: 2.304083  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.302123 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.303964  [    0/60000]\n",
      "loss: 2.302419  [ 6400/60000]\n",
      "loss: 2.307538  [12800/60000]\n",
      "loss: 2.309017  [19200/60000]\n",
      "loss: 2.295016  [25600/60000]\n",
      "loss: 2.292848  [32000/60000]\n",
      "loss: 2.301541  [38400/60000]\n",
      "loss: 2.294509  [44800/60000]\n",
      "loss: 2.304120  [51200/60000]\n",
      "loss: 2.302702  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.6%, Avg loss: 2.300922 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.302426  [    0/60000]\n",
      "loss: 2.301365  [ 6400/60000]\n",
      "loss: 2.305435  [12800/60000]\n",
      "loss: 2.306926  [19200/60000]\n",
      "loss: 2.295066  [25600/60000]\n",
      "loss: 2.292725  [32000/60000]\n",
      "loss: 2.300604  [38400/60000]\n",
      "loss: 2.294335  [44800/60000]\n",
      "loss: 2.302327  [51200/60000]\n",
      "loss: 2.301378  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.299781 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.301007  [    0/60000]\n",
      "loss: 2.300386  [ 6400/60000]\n",
      "loss: 2.303495  [12800/60000]\n",
      "loss: 2.305025  [19200/60000]\n",
      "loss: 2.294974  [25600/60000]\n",
      "loss: 2.292484  [32000/60000]\n",
      "loss: 2.299646  [38400/60000]\n",
      "loss: 2.294048  [44800/60000]\n",
      "loss: 2.300607  [51200/60000]\n",
      "loss: 2.300020  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 2.298625 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.299626  [    0/60000]\n",
      "loss: 2.299410  [ 6400/60000]\n",
      "loss: 2.301625  [12800/60000]\n",
      "loss: 2.303212  [19200/60000]\n",
      "loss: 2.294700  [25600/60000]\n",
      "loss: 2.292055  [32000/60000]\n",
      "loss: 2.298603  [38400/60000]\n",
      "loss: 2.293579  [44800/60000]\n",
      "loss: 2.298837  [51200/60000]\n",
      "loss: 2.298554  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.2%, Avg loss: 2.297361 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.298176  [    0/60000]\n",
      "loss: 2.298348  [ 6400/60000]\n",
      "loss: 2.299701  [12800/60000]\n",
      "loss: 2.301380  [19200/60000]\n",
      "loss: 2.294180  [25600/60000]\n",
      "loss: 2.291325  [32000/60000]\n",
      "loss: 2.297374  [38400/60000]\n",
      "loss: 2.292789  [44800/60000]\n",
      "loss: 2.296878  [51200/60000]\n",
      "loss: 2.296812  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 2.295841 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.296520  [    0/60000]\n",
      "loss: 2.297017  [ 6400/60000]\n",
      "loss: 2.297565  [12800/60000]\n",
      "loss: 2.299357  [19200/60000]\n",
      "loss: 2.293188  [25600/60000]\n",
      "loss: 2.290089  [32000/60000]\n",
      "loss: 2.295786  [38400/60000]\n",
      "loss: 2.291509  [44800/60000]\n",
      "loss: 2.294552  [51200/60000]\n",
      "loss: 2.294600  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 17.3%, Avg loss: 2.293929 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#model=Combined_modelInt(M1,M2)\n",
    "model = Combined_modelBruh(M1, M4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHJteOC4ef01"
   },
   "source": [
    "Further Reading\n",
    "-----------------------\n",
    "- `Loss Functions <https://pytorch.org/docs/stable/nn.html#loss-functions>`_\n",
    "- `torch.optim <https://pytorch.org/docs/stable/optim.html>`_\n",
    "- `Warmstart Training a Model <https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html>`_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Sy8cfD2eAy3U"
   },
   "outputs": [],
   "source": [
    "\n",
    "0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of optimization_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
